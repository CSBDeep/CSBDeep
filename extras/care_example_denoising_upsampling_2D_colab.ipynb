{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "care_example_2D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O9hLSEz4QF56"
      },
      "source": [
        "\n",
        "# Simple Tutorial for Content-Aware Image Restoration (CARE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hj3TGQRwQF6B"
      },
      "source": [
        "# Installation of dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bdRcte1HQF6C"
      },
      "source": [
        "\n",
        "To facilitate the training data generation as well as construction and training of the different neural enetwork architectures, we will use **CSBDeep**, a Python library for 2D/3D content-aware image restorations with convolutional neural networks (CARE). \n",
        "\n",
        "![](imgs/logo.png)\n",
        "\n",
        "This library provides for example:    \n",
        " \n",
        "* predefined architectures (residual U-Nets)\n",
        "* denoising, isotropic reconstruction, upsampling (2D/3D) workflows\n",
        "* data preprocessing/training data generation (e.g. normalisation, sampling)\n",
        "* tiled predictions\n",
        "* Export of trained models to Fiji  \n",
        "\n",
        "A full documentation can be found at  \n",
        "http://csbdeep.bioimagecomputing.com/doc/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyhNmEz7ACTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VKW7-C6hQF6C"
      },
      "source": [
        "The following would install the package locally: \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EE41zXftQF6D",
        "colab": {}
      },
      "source": [
        "!pip install csbdeep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WWxAbkt3QF6F",
        "colab": {}
      },
      "source": [
        "import csbdeep"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EoziY7haQF6G"
      },
      "source": [
        "\n",
        "### We are done with setting up! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gx52G_OoQF56"
      },
      "source": [
        "# Use case: Restoration of low SNR 2D cell images\n",
        "\n",
        "\n",
        "\n",
        "In this exercise we demonstrate how to use [CARE](http://csbdeep.bioimagecomputing.com/) for a simple 2D denoising task, where corresponding pairs of low and high signal-to-noise ratio (SNR) images of cells are available. \n",
        "The high SNR images are acquistions of Human U2OS cells taken from the [Broad Bioimage Benchmark Collection](https://data.broadinstitute.org/bbbc/BBBC006/) and the low SNR images were created by synthetically adding *strong read-out and shot-noise* and applying *pixel binning* of 2x2, thus mimicking acquisitions at a very low light level.   \n",
        "\n",
        "![](imgs/denoising_binning_overview.png)\n",
        "\n",
        "Each image pair should be registered, which in a real application setting is best achieved by acquiring both stacks _interleaved_, i.e. as different channels that correspond to the different exposure/laser settings. \n",
        "\n",
        "Since the image pairs were synthetically created in this example, they are already aligned perfectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g4WL8kLkQF57"
      },
      "source": [
        "### Download the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-iKFXj6xQF57",
        "colab": {}
      },
      "source": [
        "!wget http://idisk-srv1.mpi-cbg.de/~mweigert/snr_7_binning_2.zip\n",
        "\n",
        "!unzip snr_7_binning_2.zip -d data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H13wtjlpQF5_",
        "colab": {}
      },
      "source": [
        "!ls data/train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WDpwd9sLSDe-",
        "colab": {}
      },
      "source": [
        "from tifffile import imread\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "y = imread('data/train/GT/img_0001.tif')\n",
        "x = imread('data/train/low/img_0001.tif')\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.subplot(1,2,1);plt.imshow(x, cmap  =\"magma\");plt.colorbar();plt.title(\"low\")\n",
        "plt.subplot(1,2,2);plt.imshow(y, cmap  =\"magma\");plt.colorbar();plt.title(\"high\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bsi2Glu4QF6B"
      },
      "source": [
        "# General comment about a restoration workflow  \n",
        "\n",
        "In general, a restoration workflow will consist of the following 3 steps, which are typically split into 3 separate notebooks (see for example http://csbdeep.bioimagecomputing.com/examples/denoising3D/):\n",
        "\n",
        "\n",
        "### 1. Generation of training data \n",
        "\n",
        "* 1_datagen.ipynb\n",
        "\n",
        "### 2. Training of a restoration model \n",
        "\n",
        "* 2_training.ipynb\n",
        "    \n",
        "### 3. Prediction/Evaluation on holhold-out test set \n",
        "\n",
        "* 3_prediction.ipynb\n",
        "\n",
        "For Google Colab we concatenated these 3 notebooks into a single one, to make our lifes a little bit easier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KimHTFA9QF6H"
      },
      "source": [
        "# 1_datagen.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FNCevj5-QF6H"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Demo: Restoration of low SNR 2D cell images\n",
        "\n",
        "\n",
        "The high SNR images are acquistions of Human U2OS cells taken from the [Broad Bioimage Benchmark Collection](https://data.broadinstitute.org/bbbc/BBBC006/) and the low SNR images were created by synthetically adding *strong read-out and shot-noise* and applying *pixel binning* of 2x2, thus mimicking acquisitions at a very low light level.  \n",
        "\n",
        "![](imgs/denoising_binning_overview.png)\n",
        "\n",
        "Each image pair should be registered, which in a real application setting is best achieved by acquiring both stacks _interleaved_, i.e. as different channels that correspond to the different exposure/laser settings. \n",
        "\n",
        "Since the image pairs were synthetically created in this example, they are already aligned perfectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dZB1KRidQF6I"
      },
      "source": [
        "First, we will import some python modules..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1sNgGo0EQF6I",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, unicode_literals, absolute_import, division\n",
        "import numpy as np\n",
        "from tifffile import imread\n",
        "from csbdeep.utils import download_and_extract_zip_file, plot_some\n",
        "from csbdeep.data import RawData, create_patches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZXg187oMQF6L"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Example data\n",
        "\n",
        "The example data consists of low-SNR and high-SNR 2D images of human U2OS cells.  \n",
        "Note that `GT` stands for [ground truth](https://en.wikipedia.org/wiki/Ground_truth) and represents high signal-to-noise ratio (SNR) stacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RsFqG_KcQF6M",
        "colab": {}
      },
      "source": [
        "!ls data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wFAs9Et9QF6P"
      },
      "source": [
        "As we can see, the data set is already split into a **train** and **test** set, each containing low SNR (\"low\") and corresponding high SNR (\"GT\") images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cP_8aKp5QF6P"
      },
      "source": [
        "We can plot some training images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xegjTgRlQF6Q",
        "colab": {}
      },
      "source": [
        "y = imread('data/train/GT/img_0010.tif')\n",
        "x = imread('data/train/low/img_0010.tif')\n",
        "print('image size =', x.shape)\n",
        "\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x, cmap  =\"magma\")\n",
        "plt.colorbar()\n",
        "plt.title(\"low\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y, cmap  =\"magma\")\n",
        "plt.colorbar()\n",
        "plt.title(\"high\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y2uKesB9QF6S"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Generate training data for CARE\n",
        "\n",
        "We first need to create a `RawData` object, which defines how to get the pairs of low/high SNR stacks and the semantics of each axis (e.g. which one is considered a color channel, etc.).\n",
        "\n",
        "Here we have two folders \"low\" and \"GT\", where corresponding low and high-SNR stacks are TIFF images with identical filenames.  \n",
        "For this case, we can simply use `RawData.from_folder` and set `axes = 'YX'` to indicate the semantic order of the image axes (i.e. we have 2 dimensional images in standard xy layout). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EFhxkEkyQF6S",
        "colab": {}
      },
      "source": [
        "raw_data = RawData.from_folder (\n",
        "    basepath    = 'data/train',\n",
        "    source_dirs = ['low'],\n",
        "    target_dir  = 'GT',\n",
        "    axes        = 'YX',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "owfkKtIqQF6V"
      },
      "source": [
        "From corresponding stacks, the function `create_patches` will now generate lots of paired patches that will be used for training the CARE model later.\n",
        "\n",
        "`create_patches` returns values `(X, Y, XY_axes)`.\n",
        "By convention, the variable name `X` (or `x`) refers to an input variable for a machine learning model, whereas `Y` (or `y`) indicates an output variable.\n",
        "\n",
        "\n",
        "As a general rule, use a *patch size* that is a power of two along all axis, or at least divisible by 8. For this example we will use patches of size (128x128).\n",
        "\n",
        "An important aspect is *data normalization*, i.e. the rescaling of corresponding patches to a dynamic range of ~ (0,1). By default, this is automatically provided via percentile normalization, which can be adapted if needed. \n",
        "\n",
        "By default, patches are sampled from *non-background regions* i.e. that are above a relative threshold that can be given in the function below. We will disable this for the current example as most images region already contain foreground pixels and thus set the threshold to 0.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QdFU_1YIQF6V",
        "colab": {}
      },
      "source": [
        "from csbdeep.data import no_background_patches, norm_percentiles, sample_percentiles\n",
        "\n",
        "X, Y, XY_axes = create_patches (\n",
        "    raw_data            = raw_data,\n",
        "    patch_size          = (128,128),\n",
        "    patch_filter        = no_background_patches(0),\n",
        "    n_patches_per_image = 2,\n",
        "    save_file           = 'data/my_training_data.npz',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ymdUdxt3QF6X",
        "colab": {}
      },
      "source": [
        "assert X.shape == Y.shape\n",
        "print(\"shape of X,Y =\", X.shape)\n",
        "print(\"axes  of X,Y =\", XY_axes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6jX-R7ovQF6Z"
      },
      "source": [
        "## Show some example patches\n",
        "\n",
        "This shows the some of the generated patch pairs (even rows: *input*, odd rows: *target*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "702lesPZQF6a",
        "colab": {}
      },
      "source": [
        "for i in range(2):\n",
        "    plt.figure(figsize=(16,4))\n",
        "    sl = slice(8*i, 8*(i+1)), 0\n",
        "    plot_some(X[sl],Y[sl],title_list=[np.arange(sl[0].start,sl[0].stop)])\n",
        "    plt.show()\n",
        "None;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UvWAevj1QF6b",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uAZuB_Y3QF6d"
      },
      "source": [
        "# 2_training.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fT_uINeaQF6d"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Demo: Restoration of low SNR 2D cell images\n",
        "\n",
        "This notebook demonstrates training a CARE model for a 2D denoising task, assuming that training data was already generated via [1_datagen.ipynb](1_datagen.ipynb) and has been saved to disk to the file ``data/my_training_data.npz``.\n",
        "\n",
        "Note that training a neural network for actual use should be done on more (representative) data and with more training time.\n",
        "\n",
        "More Documentation is available at http://csbdeep.bioimagecomputing.com/doc/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VhXZQhQ7QF6e",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, unicode_literals, absolute_import, division\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # or 1 or 2 or 3\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from tifffile import imread\n",
        "from csbdeep.utils import axes_dict, plot_some, plot_history\n",
        "from csbdeep.utils.tf import limit_gpu_memory\n",
        "from csbdeep.io import load_training_data\n",
        "from csbdeep.models import Config, CARE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2HksGcDhQF6f"
      },
      "source": [
        "**Important**: As the TensorFlow backend uses all available GPU memory by default, please make sure that all other notebooks that used the GPU (e.g. training/prediction notebooks) are shut down before running this notebook. This can be done via the \"Running\" tab in the main \"Home\" notebook server page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oDnFsDOFQF6g"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Training and validation data\n",
        "\n",
        "Here we load the data patches generated via [1_datagen.ipynb](1_datagen.ipynb), and split them into 90% actual training data and 10% *validation data*. The latter is used during model training as independent indicator of the restoration accuracy. Model performance on the training data is often better than on the validation data, in which case the model is *overfitting*. Monitoring the validation performance gives us a chance to detect that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0fY5XXI3QF6g",
        "colab": {}
      },
      "source": [
        "(X,Y), (X_val,Y_val), axes = load_training_data('data/my_training_data.npz', validation_split=0.1, verbose=True)\n",
        "\n",
        "c = axes_dict(axes)['C']\n",
        "n_channel_in, n_channel_out = X.shape[c], Y.shape[c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tv6fQlzSQF6j",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plot_some(X_val[:5],Y_val[:5])\n",
        "plt.suptitle('5 example validation patches (top row: source, bottom row: target)');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5iuTn7U2QF6l"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# CARE model\n",
        "\n",
        "\n",
        "Before we construct the actual CARE model, we have to define its configuration via a `Config` object, which includes \n",
        "* parameters of the underlying neural network (a rather small 2D U-Net with 2x max pooling),\n",
        "* the learning rate,\n",
        "* the number of parameter updates per epoch,\n",
        "* the loss function, and\n",
        "* whether the model is probabilistic or not.\n",
        "\n",
        "![](imgs/carenet.png)\n",
        "\n",
        "The defaults should be sensible in many cases, so a change should only be necessary if the training process fails.  \n",
        "\n",
        "---\n",
        "\n",
        "<span style=\"color:red\">**Important**</span>: Note that for this notebook we use a very small number of update steps per epoch for immediate feedback, whereas this number should be increased considerably (e.g. `train_steps_per_epoch=400`) to obtain a well-trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tv9IbRL3QF6m",
        "colab": {}
      },
      "source": [
        "config = Config(axes, n_channel_in, n_channel_out, train_batch_size=8, train_steps_per_epoch=50, train_epochs = 200)\n",
        "print(config)\n",
        "vars(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ft_5elE5QF6o"
      },
      "source": [
        "We now create a CARE model with the chosen configuration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XvgWB9I-QF6o",
        "colab": {}
      },
      "source": [
        "model = CARE(config, 'my_model', basedir='models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JXd0ZEFQF6r",
        "colab": {}
      },
      "source": [
        "model.keras_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9-4ExTanQF6t"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Training\n",
        "\n",
        "Training the model may take some time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uOUoqzYAifj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=. --port 6008"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3oFWdMa3QF6u",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "history = model.train(X,Y, validation_data=(X_val,Y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "06smoJWtQF6v"
      },
      "source": [
        "Plot final training history (available in TensorBoard during training):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rbYeUHY7QF6v",
        "colab": {}
      },
      "source": [
        "print(sorted(list(history.history.keys())))\n",
        "plt.figure(figsize=(16,5))\n",
        "plot_history(history,['loss','val_loss'],['mse','val_mse','mae','val_mae']);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dAIV1O2-QF6x"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "Example results for validation images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yqVBbL8HQF6x",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,12))\n",
        "_P = model.keras_model.predict(X_val[-5:])\n",
        "if config.probabilistic:\n",
        "    _P = _P[...,:(_P.shape[-1]//2)]\n",
        "plot_some(X_val[-5:],Y_val[-5:],_P,pmax=99.5)\n",
        "plt.suptitle('5 example validation patches\\n'      \n",
        "             'top row: input (source),  '          \n",
        "             'middle row: target (ground truth),  '\n",
        "             'bottom row: predicted from source');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hdn01GyeQF63"
      },
      "source": [
        "# 3_prediction.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "87_o8Zt4QF64"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Demo: Restoration of low SNR 2D cell images\n",
        "\n",
        " This notebook demonstrates applying a CARE model for a 2D denoising task, assuming that training was already completed via [2_training.ipynb](2_training.ipynb).  \n",
        "The trained model is assumed to be located in the folder `models` with the name `my_model`.\n",
        "\n",
        "More Documentation is available at http://csbdeep.bioimagecomputing.com/doc/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QkOtP9q1QF64",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, unicode_literals, absolute_import, division\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "from tifffile import imread\n",
        "from csbdeep.utils import Path, download_and_extract_zip_file, plot_some\n",
        "from csbdeep.io import save_tiff_imagej_compatible\n",
        "from csbdeep.models import CARE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UuiwAHzNQF66"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Raw low-SNR image and associated high-SNR ground truth\n",
        "\n",
        "Plot the test image (with associated ground truth) and define its image axes, which will be needed later for CARE prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hnsdkUfbQF66",
        "colab": {}
      },
      "source": [
        "y = imread('data/test/GT/img_0010.tif')\n",
        "x = imread('data/test/low/img_0010.tif')\n",
        "\n",
        "axes = 'YX'\n",
        "print('image size =', x.shape)\n",
        "print('image axes =', axes)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(x, cmap  =\"magma\")\n",
        "plt.colorbar()\n",
        "plt.title(\"low\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y, cmap  =\"magma\")\n",
        "plt.colorbar()\n",
        "plt.title(\"high\");\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0rrgGmrwQF68"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# CARE model\n",
        "\n",
        "Load trained model (located in base directory `models` with name `my_model`) from disk.  \n",
        "The configuration was saved during training and is automatically loaded when `CARE` is initialized with `config=None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KvYtQP4BQF69",
        "colab": {}
      },
      "source": [
        "model = CARE(config=None, name='my_model', basedir='models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SN3QAPxkQF6-"
      },
      "source": [
        "## Apply CARE network to raw image\n",
        "\n",
        "Predict the restored image (image will be successively split into smaller tiles if there are memory issues)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HD2ekr3xQF6_",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "restored = model.predict(x, axes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oiQYSaAyQF7A"
      },
      "source": [
        "Alternatively, one can directly set `n_tiles` to an appropriate value to avoid the time overhead from multiply retries in case of memory issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SjZvfZfdQF7A"
      },
      "source": [
        "<hr style=\"height:2px;\">\n",
        "\n",
        "# Raw low/high-SNR image and denoised image via CARE network\n",
        "\n",
        "Plot the test image pair and the predicted restored image (middle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uOgEh2wUQF7B",
        "colab": {}
      },
      "source": [
        "from csbdeep.utils import normalize\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "plot_some(np.stack([x,restored,y]),\n",
        "          title_list=[['low','CARE','GT']], \n",
        "          pmin=2,pmax=99.8);\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "for _x,_name in zip((x,restored,y),('low','CARE','GT')):\n",
        "    plt.plot(normalize(_x,1,99.7)[180], label = _name, lw = 2)\n",
        "plt.legend();\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QB-V7IyHVcDO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}